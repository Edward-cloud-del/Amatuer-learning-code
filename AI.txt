🚀 FRAMESENSE AI + OCR INTEGRATION - OPTIMERAD PLAN
====================================================

📋 VERKLIG NUVARANDE STATUS (Dec 2024):
✅ Komplett screen selection system (React + HTML overlays)
✅ Screenshot capture med optimized caching
✅ ChatBox integration med image context
✅ AI integration infrastructure (mock, men OpenAI key ready)
✅ State management (Zustand) och window management
✅ Permission system för macOS
✅ Optimization system med pooling och screenshot caching
✅ Progress indicators och UX komponenter
✅ TypeScript interfaces för AI/OCR
✅ Error handling och graceful degradation

🎯 VID DETTA LÄGE SAKNAS BARA:
❌ Riktig OpenAI Vision API integration (mock → real)
❌ Tesseract OCR implementation (kommenterad i Cargo.toml)
❌ Säker API key storage system
❌ Processing router (intelligent AI vs OCR beslut)
❌ Settings UI för API keys och konfiguration
❌ Result synthesis för hybrid AI + OCR responses

===============================================
🎯 RISK-OPTIMERAD IMPLEMENTATION - 2.5 VECKOR
===============================================

🚨 KRITISKA INSIKTER FRÅN ANALYS:
• OCR är ENKLARE än AI integration (bara uncomment + code)
• Tesseract compilation är STØRSTE RISKEN - testa först
• API key chain är DEPENDENCY BLOCKER för AI
• Image compression är KRITISKT för OpenAI (20MB limit)
• Cost protection MÅSTE finnas från början

🗓️ VECKA 1: LOW-RISK HIGH-VALUE (OCR FÖRST)
===========================================

🔧 DAG 1: RISK MITIGATION - TESTA TESSERACT COMPILATION
──────────────────────────────────────────────────────
HÖGSTA PRIORITET: Verifiera att Tesseract fungerar på systemet

STEG 1A: Testa Tesseract installation
```bash
# Uncommenta i Cargo.toml:
tesseract = "0.13"

# Testa compilation:
cargo build
```

STEG 1B: Minimalt OCR test
```rust
// src-tauri/src/ocr/mod.rs - ENKEL VERSION FÖRST
use tesseract::Tesseract;

pub struct OCRService;

impl OCRService {
    pub fn test_ocr() -> Result<String, String> {
        match Tesseract::new(None, Some("eng")) {
            Ok(_) => Ok("✅ Tesseract ready!".to_string()),
            Err(e) => Err(format!("❌ Tesseract failed: {}", e))
        }
    }
}
```

🔧 DAG 2-3: OCR WORKING IMPLEMENTATION
────────────────────────────────────
Nu när vi vet att Tesseract fungerar:

IMPLEMENTERA: src-tauri/src/ocr/mod.rs
```rust
use tesseract::Tesseract;
use image::DynamicImage;
use base64::Engine;

pub struct OCRService {
    tesseract: Tesseract,
}

impl OCRService {
    pub fn new() -> Result<Self, String> {
        let tesseract = Tesseract::new(None, Some("eng"))
            .map_err(|e| format!("Failed to initialize Tesseract: {}", e))?;
        
        Ok(Self { tesseract })
    }
    
    pub fn extract_text(&mut self, image_data: &str) -> Result<OCRResult, String> {
        // Remove data:image/png;base64, prefix if exists
        let base64_data = if image_data.starts_with("data:image") {
            image_data.split(',').nth(1).unwrap_or(image_data)
        } else {
            image_data
        };
        
        // Decode base64 image
        let image_bytes = base64::engine::general_purpose::STANDARD
            .decode(base64_data)
            .map_err(|e| format!("Failed to decode image: {}", e))?;
        
        // Load image
        let img = image::load_from_memory(&image_bytes)
            .map_err(|e| format!("Failed to load image: {}", e))?
            .to_rgba8();
        
        // Set image for OCR
        self.tesseract
            .set_image_from_mem(&img.as_raw())
            .map_err(|e| format!("Failed to set image: {}", e))?;
        
        // Extract text
        let text = self.tesseract
            .get_text()
            .map_err(|e| format!("Failed to extract text: {}", e))?;
        
        let confidence = self.tesseract.mean_text_conf() as f32 / 100.0;
        
        Ok(OCRResult {
            text: text.trim().to_string(),
            confidence,
            has_text: !text.trim().is_empty() && confidence > 0.3,
        })
    }
}

#[derive(Clone, serde::Serialize, serde::Deserialize, Debug)]
pub struct OCRResult {
    pub text: String,
    pub confidence: f32,
    pub has_text: bool,
}
```

RUST COMMAND (main.rs):
```rust
mod ocr;
use ocr::{OCRService, OCRResult};

// Global OCR service (reuse instance for performance)
static mut OCR_SERVICE: Option<std::sync::Mutex<OCRService>> = None;
static OCR_INIT: std::sync::Once = std::sync::Once::new();

#[tauri::command]
async fn extract_text_ocr(image_data: String) -> Result<OCRResult, String> {
    unsafe {
        OCR_INIT.call_once(|| {
            if let Ok(service) = OCRService::new() {
                OCR_SERVICE = Some(std::sync::Mutex::new(service));
            }
        });
        
        if let Some(ref service_mutex) = OCR_SERVICE {
            let mut service = service_mutex.lock().unwrap();
            service.extract_text(&image_data)
        } else {
            Err("OCR service not initialized".to_string())
        }
    }
}
```

🔧 DAG 4-5: OCR UI INTEGRATION
────────────────────────────
UPPDATERA App.tsx för OCR testing:
```typescript
// Lägg till OCR test button temporarily
const testOCR = async () => {
  if (selectedImageForAI) {
    try {
      const result = await invoke('extract_text_ocr', { imageData: selectedImageForAI });
      console.log('📝 OCR Result:', result);
      setAiResponse(`📝 **OCR Results**\n\n**Text Found:** ${result.text}\n**Confidence:** ${Math.round(result.confidence * 100)}%`);
    } catch (error) {
      console.error('❌ OCR failed:', error);
      setAiResponse(`❌ OCR Error: ${error}`);
    }
  }
};
```

RESULTAT VECKA 1: ✅ Working OCR system, risk mitigated!

🗓️ VECKA 2: PARALLELL AI + SÄKERHETS-IMPLEMENTATION  
==================================================

🔧 DAG 1-2: API KEY STORAGE (PARALLEL MED AI PREP)
─────────────────────────────────────────────────
Eftersom OCR redan fungerar, kan vi fokusera på AI infrastructure:

RUST COMMANDS (main.rs):
```rust
use serde_json::Value;
use std::path::PathBuf;

#[tauri::command]
async fn store_api_key(key: String) -> Result<(), String> {
    // Validate API key format first
    if !key.starts_with("sk-") || key.len() < 20 {
        return Err("Invalid API key format".to_string());
    }
    
    // Get app data directory
    let app_data_dir = tauri::api::path::app_data_dir(&tauri::Config::default())
        .ok_or("Failed to get app data directory")?;
    
    let key_file = app_data_dir.join("openai_key.json");
    
    // Simple encryption (base64 + obfuscation)
    let encoded_key = base64::engine::general_purpose::STANDARD.encode(key.as_bytes());
    let data = serde_json::json!({
        "api_key": encoded_key,
        "created_at": chrono::Utc::now().timestamp()
    });
    
    // Create directory if it doesn't exist
    std::fs::create_dir_all(&app_data_dir).map_err(|e| e.to_string())?;
    
    // Write to file
    std::fs::write(key_file, data.to_string()).map_err(|e| e.to_string())?;
    
    println!("🔐 API key stored securely");
    Ok(())
}

#[tauri::command]
async fn get_api_key() -> Result<Option<String>, String> {
    let app_data_dir = tauri::api::path::app_data_dir(&tauri::Config::default())
        .ok_or("Failed to get app data directory")?;
    
    let key_file = app_data_dir.join("openai_key.json");
    
    if !key_file.exists() {
        return Ok(None);
    }
    
    let content = std::fs::read_to_string(key_file).map_err(|e| e.to_string())?;
    let data: Value = serde_json::from_str(&content).map_err(|e| e.to_string())?;
    
    if let Some(encoded_key) = data["api_key"].as_str() {
        let decoded = base64::engine::general_purpose::STANDARD
            .decode(encoded_key)
            .map_err(|e| e.to_string())?;
        let key = String::from_utf8(decoded).map_err(|e| e.to_string())?;
        Ok(Some(key))
    } else {
        Ok(None)
    }
}

#[tauri::command]
async fn remove_api_key() -> Result<(), String> {
    let app_data_dir = tauri::api::path::app_data_dir(&tauri::Config::default())
        .ok_or("Failed to get app data directory")?;
    
    let key_file = app_data_dir.join("openai_key.json");
    
    if key_file.exists() {
        std::fs::remove_file(key_file).map_err(|e| e.to_string())?;
    }
    
    Ok(())
}
```

🔧 DAG 3: OPENAI SERVICE MED COST PROTECTION
──────────────────────────────────────────
SKAPA: src/services/openai-service.ts
```typescript
import OpenAI from 'openai';

interface UsageTracker {
  requestCount: number;
  dailyLimit: number;
  lastReset: string;
}

export class OpenAIService {
  private client: OpenAI;
  private usageTracker: UsageTracker;

  constructor(apiKey: string) {
    this.client = new OpenAI({ 
      apiKey,
      dangerouslyAllowBrowser: true // Since we're in Tauri, not browser
    });
    
    // Load or initialize usage tracking
    const today = new Date().toDateString();
    const saved = localStorage.getItem('openai_usage');
    
    if (saved) {
      const parsed = JSON.parse(saved);
      if (parsed.lastReset === today) {
        this.usageTracker = parsed;
      } else {
        this.usageTracker = { requestCount: 0, dailyLimit: 50, lastReset: today };
      }
    } else {
      this.usageTracker = { requestCount: 0, dailyLimit: 50, lastReset: today };
    }
  }

  async analyzeImageWithText(imageData: string, userQuery: string): Promise<string> {
    // Cost protection check
    if (this.usageTracker.requestCount >= this.usageTracker.dailyLimit) {
      throw new Error(`Daily limit of ${this.usageTracker.dailyLimit} requests reached. Limit resets tomorrow.`);
    }

    // Image compression check (OpenAI 20MB limit)
    const imageSizeKB = (imageData.length * 0.75) / 1024;
    if (imageSizeKB > 15000) { // 15MB safety margin
      throw new Error(`Image too large (${Math.round(imageSizeKB)}KB). Max 15MB. Try selecting a smaller area.`);
    }

    try {
      const response = await this.client.chat.completions.create({
        model: "gpt-4-vision-preview",
        messages: [{
          role: "user",
          content: [
            { 
              type: "text", 
              text: `${userQuery}\n\nPlease analyze this screenshot and provide helpful insights.` 
            },
            { 
              type: "image_url", 
              image_url: { url: imageData }
            }
          ]
        }],
        max_tokens: 1000,
        temperature: 0.7
      });

      // Update usage tracking
      this.usageTracker.requestCount++;
      localStorage.setItem('openai_usage', JSON.stringify(this.usageTracker));

      const content = response.choices[0].message.content;
      if (!content) {
        throw new Error("No response from OpenAI");
      }

      console.log(`✅ OpenAI request successful (${this.usageTracker.requestCount}/${this.usageTracker.dailyLimit} today)`);
      return content;

    } catch (error: any) {
      // Handle specific OpenAI errors
      if (error.status === 401) {
        throw new Error("Invalid API key. Please check your OpenAI API key in settings.");
      } else if (error.status === 429) {
        throw new Error("Rate limit exceeded. Please wait a moment and try again.");
      } else if (error.status === 413) {
        throw new Error("Image too large. Please select a smaller area.");
      } else {
        throw new Error(`OpenAI API error: ${error.message}`);
      }
    }
  }

  getRemainingRequests(): number {
    return this.usageTracker.dailyLimit - this.usageTracker.requestCount;
  }
}
```

🔧 DAG 4-5: SMART PROCESSING ROUTER
─────────────────────────────────
SKAPA: src/services/processing-router.ts
```typescript
import { invoke } from '@tauri-apps/api/core';
import { OpenAIService } from './openai-service';

type ProcessingStrategy = 'ocr-only' | 'ai-only' | 'hybrid-sequential' | 'auto-detect';

interface ProcessingResult {
  strategy: ProcessingStrategy;
  aiResponse?: string;
  ocrResult?: any;
  combinedResponse: string;
  confidence: number;
}

export class ProcessingRouter {
  static async processImageWithQuery(
    imageData: string, 
    userQuery: string, 
    apiKey?: string
  ): Promise<ProcessingResult> {
    
    const strategy = this.determineStrategy(userQuery, imageData, !!apiKey);
    console.log(`🎯 Selected strategy: ${strategy}`);

    switch (strategy) {
      case 'ocr-only':
        return await this.runOCROnly(imageData, userQuery);
      
      case 'ai-only':
        return await this.runAIOnly(imageData, userQuery, apiKey!);
      
      case 'hybrid-sequential':
        return await this.runHybridSequential(imageData, userQuery, apiKey!);
      
      default: // auto-detect
        return await this.runAutoDetect(imageData, userQuery, apiKey);
    }
  }

  private static determineStrategy(
    query: string, 
    imageData: string, 
    hasApiKey: boolean
  ): ProcessingStrategy {
    const queryLower = query.toLowerCase();
    
    // Text extraction keywords
    const textKeywords = ['text', 'read', 'extract', 'words', 'type', 'transcribe'];
    const isTextFocused = textKeywords.some(keyword => queryLower.includes(keyword));
    
    // Visual analysis keywords  
    const visualKeywords = ['what', 'describe', 'analyze', 'explain', 'understand', 'see'];
    const isVisualFocused = visualKeywords.some(keyword => queryLower.includes(keyword));
    
    // Image size consideration
    const imageSize = imageData.length * 0.75; // Approximate bytes
    const isLargeImage = imageSize > 5000000; // 5MB
    
    // Decision logic
    if (!hasApiKey) {
      return 'ocr-only';
    } else if (isTextFocused && !isVisualFocused) {
      return 'ocr-only';
    } else if (isVisualFocused && !isTextFocused) {
      return 'ai-only';
    } else if (isLargeImage) {
      return 'ai-only'; // Avoid OCR overhead on large images
    } else {
      return 'hybrid-sequential'; // Try OCR first, then enhance with AI
    }
  }

  private static async runOCROnly(imageData: string, userQuery: string): Promise<ProcessingResult> {
    try {
      const ocrResult = await invoke('extract_text_ocr', { imageData });
      
      const response = ocrResult.has_text 
        ? `📝 **Text Extracted**\n\n${ocrResult.text}\n\n*Confidence: ${Math.round(ocrResult.confidence * 100)}%*`
        : "❌ No text detected in the selected area.";

      return {
        strategy: 'ocr-only',
        ocrResult,
        combinedResponse: response,
        confidence: ocrResult.confidence
      };
    } catch (error) {
      return {
        strategy: 'ocr-only',
        combinedResponse: `❌ OCR failed: ${error}`,
        confidence: 0
      };
    }
  }

  private static async runAIOnly(imageData: string, userQuery: string, apiKey: string): Promise<ProcessingResult> {
    try {
      const openai = new OpenAIService(apiKey);
      const aiResponse = await openai.analyzeImageWithText(imageData, userQuery);
      
      return {
        strategy: 'ai-only',
        aiResponse,
        combinedResponse: `🤖 **AI Analysis**\n\n${aiResponse}`,
        confidence: 0.9 // AI is generally high confidence
      };
    } catch (error) {
      return {
        strategy: 'ai-only',
        combinedResponse: `❌ AI analysis failed: ${error}`,
        confidence: 0
      };
    }
  }

  private static async runHybridSequential(imageData: string, userQuery: string, apiKey: string): Promise<ProcessingResult> {
    // Step 1: Run OCR first
    let ocrResult;
    try {
      ocrResult = await invoke('extract_text_ocr', { imageData });
    } catch (error) {
      console.warn('OCR failed, falling back to AI only:', error);
      return await this.runAIOnly(imageData, userQuery, apiKey);
    }

    // Step 2: Run AI with OCR context
    try {
      const openai = new OpenAIService(apiKey);
      
      const enhancedQuery = ocrResult.has_text 
        ? `${userQuery}\n\nAdditional context - OCR detected this text: "${ocrResult.text}"`
        : userQuery;
      
      const aiResponse = await openai.analyzeImageWithText(imageData, enhancedQuery);
      
      // Step 3: Combine results intelligently
      const combinedResponse = this.synthesizeResults(ocrResult, aiResponse, userQuery);
      
      return {
        strategy: 'hybrid-sequential',
        aiResponse,
        ocrResult,
        combinedResponse,
        confidence: Math.max(0.8, ocrResult.confidence) // Hybrid is high confidence
      };
    } catch (error) {
      // Fallback to OCR-only if AI fails
      console.warn('AI failed, using OCR results only:', error);
      return await this.runOCROnly(imageData, userQuery);
    }
  }

  private static async runAutoDetect(imageData: string, userQuery: string, apiKey?: string): Promise<ProcessingResult> {
    if (!apiKey) {
      return await this.runOCROnly(imageData, userQuery);
    }
    
    // Default to hybrid for best results
    return await this.runHybridSequential(imageData, userQuery, apiKey);
  }

  private static synthesizeResults(ocrResult: any, aiResponse: string, userQuery: string): string {
    if (!ocrResult.has_text) {
      return `🤖 **AI Analysis**\n\n${aiResponse}`;
    }

    return `🔄 **Hybrid Analysis**\n\n` +
           `📝 **Text Found:** ${ocrResult.text}\n` +
           `*OCR Confidence: ${Math.round(ocrResult.confidence * 100)}%*\n\n` +
           `🤖 **AI Insights:**\n${aiResponse}`;
  }
}
```

🗓️ VECKA 3: INTEGRATION & POLISH
================================

🔧 DAG 1-2: SETTINGS UI & MAIN APP INTEGRATION
─────────────────────────────────────────────
SKAPA: src/components/SettingsDialog.tsx (kompakt version)
UPPDATERA: App.tsx med Settings button och ny ProcessingRouter

🔧 DAG 3-4: ERROR HANDLING & UX POLISH
────────────────────────────────────
• User-friendly error messages
• Loading states med progress
• Usage limit indicators

🔧 DAG 5: TESTING & CLEANUP
─────────────────────────
• End-to-end testing
• Performance verification
• Remove temporary debugging code

===============================================
🎯 FÖRBÄTTRAD IMPLEMENTATION ORDER
===============================================

🚀 **DAG 1 (HÖGRISK): TESSERACT TEST**
• Uncomment Tesseract i Cargo.toml
• Testa compilation + basic OCR
• STOP om detta inte fungerar - fixa först!

⚡ **DAG 2-3: OCR IMPLEMENTATION** 
• Working OCR service
• UI integration för testing
• Immediate value utan external dependencies

🔥 **DAG 4-5: API INFRASTRUCTURE**
• API key storage (säkert)
• OpenAI service med cost protection
• Settings UI basic version

💪 **DAG 6-8: SMART PROCESSING**
• Processing router implementation
• Hybrid AI + OCR kombinationer
• Error handling och fallbacks

✨ **DAG 9-12: POLISH & TESTING**
• UI improvements
• Performance testing
• Production readiness

===============================================
🎯 KRITISKA FÖRBÄTTRINGAR I DENNA PLAN
===============================================

1. **🚨 RISK MITIGATION FÖRST:** Testa Tesseract compilation dag 1
2. **💰 COST PROTECTION:** Daily limits och image size checks
3. **⚡ IMMEDIATE VALUE:** OCR fungerar utan externa dependencies  
4. **🔄 SMART FALLBACKS:** Om AI fails → OCR, om OCR fails → AI
5. **📈 INCREMENTAL:** Varje dag levererar working functionality
6. **🛡️ FUTURE-PROOF:** Ingen konflikt med Appoptimization.txt optimeringar

===============================================
🚀 DENNA FÖRBÄTTRADE PLAN ÄR BOMBSÄKER!
===============================================

**Fördelar vs original plan:**
✅ Tesseract risk eliminerad dag 1  
✅ OCR working inom 3 dagar
✅ Cost protection från början
✅ Fallback strategies för alla scenarios
✅ Ingen dependency på external services för core functionality
✅ Parallel development för maximal effektivitet

**🎯 Success efter varje dag:**
• Dag 1: Tesseract proven working
• Dag 3: OCR extracting text från screenshots  
• Dag 5: API key management working
• Dag 8: AI + OCR hybrid system working
• Dag 12: Production-ready med polish

💪 **DETTA KOMMER DEFINITIVT FUNGERA!** 